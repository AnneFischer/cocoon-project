Epoch [1/40] Training loss: 2.4585070257
--------------------------------------------------
Epoch [2/40] Training loss: 1.2217223632
--------------------------------------------------
Epoch [3/40] Training loss: 1.1326990450
--------------------------------------------------
Epoch [4/40] Training loss: 1.0566166217
--------------------------------------------------
Epoch [5/40] Training loss: 1.0005054188
--------------------------------------------------
Epoch [6/40] Training loss: 0.9360264242
--------------------------------------------------
Epoch [7/40] Training loss: 0.8762073057
--------------------------------------------------
Epoch [8/40] Training loss: 0.8053165115
--------------------------------------------------
Epoch [9/40] Training loss: 0.6108044212
--------------------------------------------------
Epoch [10/40] Training loss: 0.4716711647
--------------------------------------------------
Epoch [11/40] Training loss: 0.3070249235
--------------------------------------------------
Epoch [12/40] Training loss: 0.3412806003
--------------------------------------------------
Epoch [13/40] Training loss: 0.1831267687
--------------------------------------------------
Epoch [14/40] Training loss: 0.1613978524
--------------------------------------------------
Epoch [15/40] Training loss: 0.1144940776
--------------------------------------------------
Epoch [16/40] Training loss: 0.1043990979
--------------------------------------------------
Epoch [17/40] Training loss: 0.0352313793
--------------------------------------------------
Epoch [18/40] Training loss: 0.0345802523
--------------------------------------------------
Epoch [19/40] Training loss: 0.0070823857
--------------------------------------------------
Epoch [20/40] Training loss: 0.0073512131
--------------------------------------------------
Epoch [21/40] Training loss: 0.0119947139
--------------------------------------------------
Epoch [22/40] Training loss: 0.0248669375
--------------------------------------------------
Epoch [23/40] Training loss: 0.0452974331
--------------------------------------------------
Epoch [24/40] Training loss: 0.1573396420
--------------------------------------------------
Epoch [25/40] Training loss: 0.0441596738
--------------------------------------------------
Epoch [26/40] Training loss: 0.0594813766
--------------------------------------------------
Epoch [27/40] Training loss: 0.0164380347
--------------------------------------------------
Epoch [28/40] Training loss: 0.0211020827
--------------------------------------------------
Epoch [29/40] Training loss: 0.0256121910
--------------------------------------------------
Epoch [30/40] Training loss: 0.0413290131
--------------------------------------------------
Epoch [31/40] Training loss: 0.0408654471
--------------------------------------------------
Epoch [32/40] Training loss: 0.1226718893
--------------------------------------------------
Epoch [33/40] Training loss: 0.0766449670
--------------------------------------------------
Epoch [34/40] Training loss: 0.1204709673
--------------------------------------------------
Epoch [35/40] Training loss: 0.0978117792
--------------------------------------------------
Epoch [36/40] Training loss: 0.0417488810
--------------------------------------------------
Epoch [37/40] Training loss: 0.0673181029
--------------------------------------------------
Epoch [38/40] Training loss: 0.2816043633
--------------------------------------------------
Epoch [39/40] Training loss: 0.0417352273
--------------------------------------------------
Epoch [40/40] Training loss: 0.1178918685
--------------------------------------------------
TRAINING COMPLETE
Model was saved at 40 epochs
Loading at epoch 40 saved model weights...
The number of correct predicted samples: 0/10
The number of correct predicted samples: 0/10
The number of correct predicted samples: 0/10
The number of correct predicted samples: 0/10
The number of correct predicted samples: 0/10
The number of correct predicted samples: 0/10
Total test pred: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Total test labels: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Total test prob: [0.0011070648, 0.0016811083, 0.0053250426, 0.11444493, 0.016566705, 0.002217302, 0.053187784, 0.26156744, 0.7932485, 0.49915266, 0.0016100714, 0.0041724076, 0.0009776726, 0.012503369, 9.389065e-05, 0.0048643346, 0.6865637, 0.05562296, 0.6883699, 0.009377584, 9.0430825e-05, 0.0262994, 0.9527139, 0.00020416208, 0.11032268, 0.007918652, 0.12889859, 0.077159725, 0.27454272, 0.074776575, 0.056362957, 0.25415966, 0.00020493206, 0.0027520994, 0.98203284, 0.016185543, 0.0021327336, 0.06251421, 0.10293422, 0.03718172, 2.7265529e-05, 0.0024411133, 0.0099267885, 0.0021393716, 0.06357023, 0.9951994, 0.08130367, 0.0025273918, 0.0035158624, 0.06634641, 0.0021458778, 0.00071707583, 0.08444808, 0.027263809, 0.008227915, 0.0015475184, 0.001989311, 0.001963153, 0.00034354735, 0.0019135571]
Precision score: 0.0
Recall score: 0.0
F1 score: 0.0
Average precision score: 0.12859288719766898
AUC score: 0.38221153846153844
[32m[I 2022-03-23 16:00:46,434][39m A new study created in memory with name: no-name-91ef4b7b-747c-4e35-974b-2915f81a172e
Params: {'learning_rate': 0.009819675921803757, 'num_hidden_units_per_layer': 64, 'weight_decay': 0.030322386851056878, 'kernel_size': 5, 'num_epochs': 15, 'drop_out': 0.4317492425820888, 'batch_size': 20, 'reduced_seq_length': 180, 'num_levels': 6, 'stride': 1, 'pos_weight': tensor([6.8261])}
The number of rows with infinite numbers that will be replaced with zero is: 0
The number of rows with NA numbers that will be replaced with zero is: 0
The number of rows with infinite numbers that will be replaced with zero is: 0
The number of rows with NA numbers that will be replaced with zero is: 0