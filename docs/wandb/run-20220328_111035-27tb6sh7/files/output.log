Epoch [1/35] Training loss: 1.2199	 Validation loss: 1.1679
--------------------------------------------------
Epoch [2/35] Training loss: 1.2097	 Validation loss: 1.1681
--------------------------------------------------
Epoch [3/35] Training loss: 1.2006	 Validation loss: 1.1683
--------------------------------------------------
Epoch [4/35] Training loss: 1.2332	 Validation loss: 1.1683
--------------------------------------------------
Epoch [5/35] Training loss: 1.2338	 Validation loss: 1.1685
--------------------------------------------------
Epoch [6/35] Training loss: 1.2140	 Validation loss: 1.1678
--------------------------------------------------
Epoch [7/35] Training loss: 1.2037	 Validation loss: 1.1675
--------------------------------------------------
Epoch [8/35] Training loss: 1.1715	 Validation loss: 1.1671
--------------------------------------------------
Epoch [9/35] Training loss: 1.1950	 Validation loss: 1.1674
--------------------------------------------------
Epoch [10/35] Training loss: 1.1864	 Validation loss: 1.1675
--------------------------------------------------
Epoch [11/35] Training loss: 1.1914	 Validation loss: 1.1681
--------------------------------------------------
Epoch [12/35] Training loss: 1.2346	 Validation loss: 1.1684
--------------------------------------------------
Epoch [13/35] Training loss: 1.1940	 Validation loss: 1.1689
--------------------------------------------------
Epoch [14/35] Training loss: 1.1919	 Validation loss: 1.1689
--------------------------------------------------
Epoch [15/35] Training loss: 1.1874	 Validation loss: 1.1692
--------------------------------------------------
Epoch [16/35] Training loss: 1.1671	 Validation loss: 1.1689
--------------------------------------------------
Epoch [17/35] Training loss: 1.1817	 Validation loss: 1.1684
--------------------------------------------------
Epoch [18/35] Training loss: 1.1810	 Validation loss: 1.1679
--------------------------------------------------
Epoch [19/35] Training loss: 1.1762	 Validation loss: 1.1676
--------------------------------------------------
Epoch [20/35] Training loss: 1.1772	 Validation loss: 1.1678
--------------------------------------------------
Epoch [21/35] Training loss: 1.1460	 Validation loss: 1.1686
--------------------------------------------------
Epoch [22/35] Training loss: 1.1746	 Validation loss: 1.1693
--------------------------------------------------
Epoch [23/35] Training loss: 1.1470	 Validation loss: 1.1692
--------------------------------------------------
Epoch [24/35] Training loss: 1.1530	 Validation loss: 1.1687
--------------------------------------------------
Epoch [25/35] Training loss: 1.1153	 Validation loss: 1.1683
--------------------------------------------------
Epoch [26/35] Training loss: 1.1185	 Validation loss: 1.1677
--------------------------------------------------
Epoch [27/35] Training loss: 1.1730	 Validation loss: 1.1670
--------------------------------------------------
Epoch [28/35] Training loss: 1.1499	 Validation loss: 1.1662
--------------------------------------------------
Epoch [29/35] Training loss: 1.1785	 Validation loss: 1.1660
--------------------------------------------------
Epoch [30/35] Training loss: 1.1220	 Validation loss: 1.1655
--------------------------------------------------
Epoch [31/35] Training loss: 1.1612	 Validation loss: 1.1661
--------------------------------------------------
Epoch [32/35] Training loss: 1.1464	 Validation loss: 1.1671
--------------------------------------------------
Epoch [33/35] Training loss: 1.1209	 Validation loss: 1.1675
--------------------------------------------------
Epoch [34/35] Training loss: 1.1319	 Validation loss: 1.1685
--------------------------------------------------
Epoch [35/35] Training loss: 1.1089	 Validation loss: 1.1700
--------------------------------------------------
TRAINING COMPLETE
Params: {'learning_rate': 1.4253394876181958e-05, 'num_hidden_units_per_layer': 96, 'weight_decay': 0.00010305485654838291, 'kernel_size': 7, 'num_epochs': 30, 'drop_out': 0.36645805169427453, 'batch_size': 40, 'reduced_seq_length': 200, 'num_levels': 5, 'stride': 1, 'pos_weight': tensor([6.8261])}
[32m[I 2022-03-28 11:11:09,848][39m Trial 10 finished with value: 1.1699931621551514 and parameters: {'learning_rate': 0.00012770770043511842, 'num_hidden_units_per_layer': 32, 'weight_decay': 0.09656077756527455, 'kernel_size': 9, 'num_epochs': 35, 'drop_out': 0.41153680403472803, 'batch_size': 30, 'reduced_seq_length': 100}. Best is trial 8 with value: 1.1622211933135986.