Epoch [1/25] Training loss: 18.0908	 Validation loss: 1.3079
--------------------------------------------------
Epoch [2/25] Training loss: 1.2668	 Validation loss: 1.1795
--------------------------------------------------
Epoch [3/25] Training loss: 1.1969	 Validation loss: 1.2079
--------------------------------------------------
Epoch [4/25] Training loss: 1.2205	 Validation loss: 1.2022
--------------------------------------------------
Epoch [5/25] Training loss: 1.2424	 Validation loss: 1.1823
--------------------------------------------------
Epoch [6/25] Training loss: 1.2253	 Validation loss: 1.1921
--------------------------------------------------
Epoch [7/25] Training loss: 1.2216	 Validation loss: 1.2025
--------------------------------------------------
Epoch [8/25] Training loss: 1.1867	 Validation loss: 1.1942
--------------------------------------------------
Epoch [9/25] Training loss: 1.2080	 Validation loss: 1.1950
--------------------------------------------------
Epoch [10/25] Training loss: 1.1544	 Validation loss: 1.1966
--------------------------------------------------
Epoch [11/25] Training loss: 1.2116	 Validation loss: 1.1990
--------------------------------------------------
Epoch [12/25] Training loss: 1.1548	 Validation loss: 1.2013
--------------------------------------------------
Epoch [13/25] Training loss: 1.1249	 Validation loss: 1.2122
--------------------------------------------------
Epoch [14/25] Training loss: 1.1799	 Validation loss: 1.2095
--------------------------------------------------
Epoch [15/25] Training loss: 1.1886	 Validation loss: 1.2073
--------------------------------------------------
Epoch [16/25] Training loss: 1.1103	 Validation loss: 1.2120
--------------------------------------------------
Epoch [17/25] Training loss: 1.1591	 Validation loss: 1.2218
--------------------------------------------------
Epoch [18/25] Training loss: 1.1949	 Validation loss: 1.2168
--------------------------------------------------
Epoch [19/25] Training loss: 1.1503	 Validation loss: 1.2286
--------------------------------------------------
Epoch [20/25] Training loss: 1.2138	 Validation loss: 1.2177
--------------------------------------------------
Epoch [21/25] Training loss: 1.1676	 Validation loss: 1.1998
--------------------------------------------------
Epoch [22/25] Training loss: 1.1620	 Validation loss: 1.1907
--------------------------------------------------
Epoch [23/25] Training loss: 1.1701	 Validation loss: 1.1794
--------------------------------------------------
Epoch [24/25] Training loss: 1.1678	 Validation loss: 1.1680
--------------------------------------------------
[32m[I 2022-03-24 09:17:52,563][39m Trial 22 finished with value: 1.1716886162757874 and parameters: {'learning_rate': 0.009020030599366619, 'num_hidden_units_per_layer': 160, 'weight_decay': 4.1078040444468215e-06, 'kernel_size': 3, 'num_epochs': 25, 'drop_out': 0.44068493308494927, 'batch_size': 50, 'reduced_seq_length': 120}. Best is trial 3 with value: 0.9335269729296366.
Epoch [25/25] Training loss: 1.1041	 Validation loss: 1.1717
--------------------------------------------------
TRAINING COMPLETE
Params: {'learning_rate': 0.04399327364507564, 'num_hidden_units_per_layer': 192, 'weight_decay': 7.936865706287872e-05, 'kernel_size': 3, 'num_epochs': 30, 'drop_out': 0.3860442275383462, 'batch_size': 40, 'reduced_seq_length': 80, 'num_levels': 5, 'stride': 1, 'pos_weight': tensor([6.8261])}
The number of rows with infinite numbers that will be replaced with zero is: 0
The number of rows with NA numbers that will be replaced with zero is: 0
The number of rows with infinite numbers that will be replaced with zero is: 0
The number of rows with NA numbers that will be replaced with zero is: 0