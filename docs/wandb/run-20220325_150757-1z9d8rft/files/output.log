Epoch [1/20] Training loss: 1.2043390423
--------------------------------------------------
Epoch [2/20] Training loss: 1.2049838081
--------------------------------------------------
Epoch [3/20] Training loss: 1.2111828402
--------------------------------------------------
Epoch [4/20] Training loss: 1.2326136380
--------------------------------------------------
Epoch [5/20] Training loss: 1.2063623741
--------------------------------------------------
Epoch [6/20] Training loss: 1.2293102667
--------------------------------------------------
Epoch [7/20] Training loss: 1.2411480099
--------------------------------------------------
Epoch [8/20] Training loss: 1.2038752437
--------------------------------------------------
Epoch [9/20] Training loss: 1.2007893100
--------------------------------------------------
Epoch [10/20] Training loss: 1.2191114277
--------------------------------------------------
Epoch [11/20] Training loss: 1.2042940855
--------------------------------------------------
Epoch [12/20] Training loss: 1.2142159492
--------------------------------------------------
Epoch [13/20] Training loss: 1.2169792876
--------------------------------------------------
Epoch [14/20] Training loss: 1.2359723374
--------------------------------------------------
Epoch [15/20] Training loss: 1.2168624550
--------------------------------------------------
Epoch [16/20] Training loss: 1.1860879436
--------------------------------------------------
Epoch [17/20] Training loss: 1.1875960082
--------------------------------------------------
Epoch [18/20] Training loss: 1.1940555722
--------------------------------------------------
Epoch [19/20] Training loss: 1.2204292417
--------------------------------------------------
Epoch [20/20] Training loss: 1.1892251372
--------------------------------------------------
TRAINING COMPLETE
Model is saved at: /Users/AFischer/Documents/PhD_onderzoek/term_preterm_database/output/model/2022-03-25_15-07_tcn_sample_entropy_final_train.pth
Model was saved at 20 epochs
Loading at epoch 20 saved model weights...
The number of correct predicted samples: 26/30
The number of correct predicted samples: 24/30
Total test pred: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Total test labels: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
Total test prob: [0.45877975, 0.4664869, 0.47206774, 0.46729976, 0.45675635, 0.4691035, 0.46842358, 0.47241613, 0.4483443, 0.46622482, 0.4592756, 0.504048, 0.46785268, 0.46788472, 0.5120041, 0.47215682, 0.48267013, 0.47115788, 0.46562898, 0.47509763, 0.46918666, 0.4712911, 0.4612219, 0.50586504, 0.49198487, 0.473588, 0.45965025, 0.46764946, 0.467494, 0.51438004, 0.46918502, 0.47237468, 0.46311933, 0.47721407, 0.47397047, 0.47482672, 0.44579926, 0.4927513, 0.49246186, 0.4718222, 0.4844695, 0.47164625, 0.46714517, 0.48274913, 0.45506632, 0.4828158, 0.47888985, 0.49958122, 0.4888582, 0.47754583, 0.46344775, 0.45886207, 0.46905062, 0.45960578, 0.4765825, 0.4562069, 0.45147845, 0.47430772, 0.49287754, 0.46739748]
Precision score: 0.25
Recall score: 0.125
F1 score: 0.16666666666666666
Average precision score: 0.1762855354948312
AUC score: 0.4567307692307693
The number of data points before removing the first 3600 data points (per rec_id) is: 10655901
The number of data points after removing the first 3600 data points (per rec_id) is: 9575901
The number of data points before removing the last 3600 data points (per rec_id) is: 9575901
The number of data points after removing the last 3600 data points (per rec_id) is: 8495901