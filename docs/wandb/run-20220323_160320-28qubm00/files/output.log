Epoch [1/45] Training loss: 1.2343	 Validation loss: 1.1831
--------------------------------------------------
Epoch [2/45] Training loss: 1.2559	 Validation loss: 1.1801
--------------------------------------------------
Epoch [3/45] Training loss: 1.2528	 Validation loss: 1.1781
--------------------------------------------------
Epoch [4/45] Training loss: 1.2503	 Validation loss: 1.1758
--------------------------------------------------
Epoch [5/45] Training loss: 1.2227	 Validation loss: 1.1739
--------------------------------------------------
Epoch [6/45] Training loss: 1.2016	 Validation loss: 1.1716
--------------------------------------------------
Epoch [7/45] Training loss: 1.2121	 Validation loss: 1.1698
--------------------------------------------------
Epoch [8/45] Training loss: 1.2290	 Validation loss: 1.1678
--------------------------------------------------
Epoch [9/45] Training loss: 1.2070	 Validation loss: 1.1662
--------------------------------------------------
Epoch [10/45] Training loss: 1.2308	 Validation loss: 1.1646
--------------------------------------------------
Epoch [11/45] Training loss: 1.2383	 Validation loss: 1.1631
--------------------------------------------------
Epoch [12/45] Training loss: 1.2172	 Validation loss: 1.1616
--------------------------------------------------
Epoch [13/45] Training loss: 1.2302	 Validation loss: 1.1603
--------------------------------------------------
Epoch [14/45] Training loss: 1.2255	 Validation loss: 1.1593
--------------------------------------------------
Epoch [15/45] Training loss: 1.2359	 Validation loss: 1.1581
--------------------------------------------------
Epoch [16/45] Training loss: 1.2250	 Validation loss: 1.1567
--------------------------------------------------
Epoch [17/45] Training loss: 1.2168	 Validation loss: 1.1560
--------------------------------------------------
Epoch [18/45] Training loss: 1.2123	 Validation loss: 1.1553
--------------------------------------------------
Epoch [19/45] Training loss: 1.2438	 Validation loss: 1.1547
--------------------------------------------------
Epoch [20/45] Training loss: 1.1903	 Validation loss: 1.1543
--------------------------------------------------
Epoch [21/45] Training loss: 1.1982	 Validation loss: 1.1536
--------------------------------------------------
Epoch [22/45] Training loss: 1.1950	 Validation loss: 1.1530
--------------------------------------------------
Epoch [23/45] Training loss: 1.1769	 Validation loss: 1.1523
--------------------------------------------------
Epoch [24/45] Training loss: 1.2062	 Validation loss: 1.1519
--------------------------------------------------
Epoch [25/45] Training loss: 1.1816	 Validation loss: 1.1512
--------------------------------------------------
Epoch [26/45] Training loss: 1.1902	 Validation loss: 1.1505
--------------------------------------------------
Epoch [27/45] Training loss: 1.2181	 Validation loss: 1.1497
--------------------------------------------------
Epoch [28/45] Training loss: 1.2152	 Validation loss: 1.1488
--------------------------------------------------
Epoch [29/45] Training loss: 1.2011	 Validation loss: 1.1483
--------------------------------------------------
Epoch [30/45] Training loss: 1.1842	 Validation loss: 1.1476
--------------------------------------------------
Epoch [31/45] Training loss: 1.2067	 Validation loss: 1.1470
--------------------------------------------------
Epoch [32/45] Training loss: 1.1855	 Validation loss: 1.1466
--------------------------------------------------
Epoch [33/45] Training loss: 1.2060	 Validation loss: 1.1464
--------------------------------------------------
Epoch [34/45] Training loss: 1.1778	 Validation loss: 1.1461
--------------------------------------------------
Epoch [35/45] Training loss: 1.2192	 Validation loss: 1.1458
--------------------------------------------------
Epoch [36/45] Training loss: 1.2073	 Validation loss: 1.1455
--------------------------------------------------
Epoch [37/45] Training loss: 1.1906	 Validation loss: 1.1451
--------------------------------------------------
Epoch [38/45] Training loss: 1.1806	 Validation loss: 1.1449
--------------------------------------------------
Epoch [39/45] Training loss: 1.2112	 Validation loss: 1.1447
--------------------------------------------------
Epoch [40/45] Training loss: 1.1999	 Validation loss: 1.1444
--------------------------------------------------
Epoch [41/45] Training loss: 1.1796	 Validation loss: 1.1442
--------------------------------------------------
Epoch [42/45] Training loss: 1.2252	 Validation loss: 1.1440
--------------------------------------------------
Epoch [43/45] Training loss: 1.1972	 Validation loss: 1.1439
--------------------------------------------------
[32m[I 2022-03-23 16:04:11,275][39m Trial 1 finished with value: 1.143883506457011 and parameters: {'learning_rate': 1.6825278868116756e-05, 'num_hidden_units_per_layer': 64, 'weight_decay': 3.2095058177057514e-06, 'kernel_size': 5, 'num_epochs': 45, 'drop_out': 0.4307148452826485, 'batch_size': 40, 'reduced_seq_length': 80}. Best is trial 1 with value: 1.143883506457011.
Epoch [44/45] Training loss: 1.1839	 Validation loss: 1.1441
--------------------------------------------------
Epoch [45/45] Training loss: 1.2199	 Validation loss: 1.1439
--------------------------------------------------
TRAINING COMPLETE
Params: {'learning_rate': 0.0005242599197862023, 'num_hidden_units_per_layer': 64, 'weight_decay': 1.88784196132869e-05, 'kernel_size': 9, 'num_epochs': 50, 'drop_out': 0.12606541196951057, 'batch_size': 30, 'reduced_seq_length': 120, 'num_levels': 4, 'stride': 1, 'pos_weight': tensor([6.8261])}
The number of rows with infinite numbers that will be replaced with zero is: 0
The number of rows with NA numbers that will be replaced with zero is: 0
The number of rows with infinite numbers that will be replaced with zero is: 0
The number of rows with NA numbers that will be replaced with zero is: 0