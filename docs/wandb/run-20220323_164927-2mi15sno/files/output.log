Epoch [1/40] Training loss: 1.2210	 Validation loss: 1.1908
--------------------------------------------------
Epoch [2/40] Training loss: 1.2383	 Validation loss: 1.1904
--------------------------------------------------
Epoch [3/40] Training loss: 1.2315	 Validation loss: 1.1902
--------------------------------------------------
Epoch [4/40] Training loss: 1.2045	 Validation loss: 1.1899
--------------------------------------------------
Epoch [5/40] Training loss: 1.2019	 Validation loss: 1.1896
--------------------------------------------------
Epoch [6/40] Training loss: 1.2528	 Validation loss: 1.1894
--------------------------------------------------
Epoch [7/40] Training loss: 1.2200	 Validation loss: 1.1892
--------------------------------------------------
Epoch [8/40] Training loss: 1.2210	 Validation loss: 1.1890
--------------------------------------------------
Epoch [9/40] Training loss: 1.2147	 Validation loss: 1.1886
--------------------------------------------------
Epoch [10/40] Training loss: 1.2175	 Validation loss: 1.1884
--------------------------------------------------
Epoch [11/40] Training loss: 1.2133	 Validation loss: 1.1882
--------------------------------------------------
Epoch [12/40] Training loss: 1.2407	 Validation loss: 1.1880
--------------------------------------------------
Epoch [13/40] Training loss: 1.1971	 Validation loss: 1.1878
--------------------------------------------------
Epoch [14/40] Training loss: 1.2155	 Validation loss: 1.1876
--------------------------------------------------
Epoch [15/40] Training loss: 1.2175	 Validation loss: 1.1874
--------------------------------------------------
Epoch [16/40] Training loss: 1.2500	 Validation loss: 1.1873
--------------------------------------------------
Epoch [17/40] Training loss: 1.2257	 Validation loss: 1.1870
--------------------------------------------------
Epoch [18/40] Training loss: 1.2475	 Validation loss: 1.1869
--------------------------------------------------
Epoch [19/40] Training loss: 1.2231	 Validation loss: 1.1866
--------------------------------------------------
Epoch [20/40] Training loss: 1.2127	 Validation loss: 1.1865
--------------------------------------------------
Epoch [21/40] Training loss: 1.2193	 Validation loss: 1.1863
--------------------------------------------------
Epoch [22/40] Training loss: 1.2230	 Validation loss: 1.1861
--------------------------------------------------
Epoch [23/40] Training loss: 1.2355	 Validation loss: 1.1859
--------------------------------------------------
Epoch [24/40] Training loss: 1.2324	 Validation loss: 1.1856
--------------------------------------------------
Epoch [25/40] Training loss: 1.2411	 Validation loss: 1.1854
--------------------------------------------------
Epoch [26/40] Training loss: 1.2229	 Validation loss: 1.1852
--------------------------------------------------
Epoch [27/40] Training loss: 1.2261	 Validation loss: 1.1852
--------------------------------------------------
Epoch [28/40] Training loss: 1.2179	 Validation loss: 1.1851
--------------------------------------------------
Epoch [29/40] Training loss: 1.2083	 Validation loss: 1.1849
--------------------------------------------------
Epoch [30/40] Training loss: 1.1953	 Validation loss: 1.1847
--------------------------------------------------
Epoch [31/40] Training loss: 1.1986	 Validation loss: 1.1845
--------------------------------------------------
Epoch [32/40] Training loss: 1.2156	 Validation loss: 1.1843
--------------------------------------------------
Epoch [33/40] Training loss: 1.2090	 Validation loss: 1.1842
--------------------------------------------------
Epoch [34/40] Training loss: 1.2298	 Validation loss: 1.1840
--------------------------------------------------
Epoch [35/40] Training loss: 1.2632	 Validation loss: 1.1839
--------------------------------------------------
Epoch [36/40] Training loss: 1.2418	 Validation loss: 1.1839
--------------------------------------------------
Epoch [37/40] Training loss: 1.2100	 Validation loss: 1.1837
--------------------------------------------------
Epoch [38/40] Training loss: 1.2192	 Validation loss: 1.1837
--------------------------------------------------
Epoch [39/40] Training loss: 1.2070	 Validation loss: 1.1835
--------------------------------------------------
Epoch [40/40] Training loss: 1.2126	 Validation loss: 1.1835
--------------------------------------------------
TRAINING COMPLETE
Params: {'learning_rate': 0.00013840790502197897, 'num_hidden_units_per_layer': 32, 'weight_decay': 5.614870628067318e-06, 'kernel_size': 9, 'num_epochs': 10, 'drop_out': 0.3663077502107681, 'batch_size': 40, 'reduced_seq_length': 40, 'num_levels': 3, 'stride': 1, 'pos_weight': tensor([6.8261])}
[32m[I 2022-03-23 16:49:45,139][39m Trial 1 finished with value: 1.1834737459818523 and parameters: {'learning_rate': 1.8345410487918485e-05, 'num_hidden_units_per_layer': 32, 'weight_decay': 2.5085499954236333e-05, 'kernel_size': 3, 'num_epochs': 40, 'drop_out': 0.46878044459585066, 'batch_size': 20, 'reduced_seq_length': 20}. Best is trial 1 with value: 1.1834737459818523.
The number of rows with infinite numbers that will be replaced with zero is: 0
The number of rows with NA numbers that will be replaced with zero is: 0
The number of rows with infinite numbers that will be replaced with zero is: 0
The number of rows with NA numbers that will be replaced with zero is: 0